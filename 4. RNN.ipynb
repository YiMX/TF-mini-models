{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN forward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "num_cell = 128\n",
    "\n",
    "hidden_units = 30\n",
    "\n",
    "def rnn_forward(inputs):\n",
    "    # Define the cell, can be either LSTM or GRU, or the peephole implementation.\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_cell)\n",
    "    \n",
    "    # Here we define a dynamic rnn. It builds a dynamic graph such that every time we can pass a batch with varied lengths.\n",
    "    # time_major=False means the inputs must be [batch_size, time_step, features], otherwise [time_step, batch_size, features]\n",
    "    # sequence_length is not necessarily to be specified.\n",
    "    rnn_outputs, states = tf.nn.dynamic_rnn(cell=cell,\n",
    "                                            inputs=inputs,\n",
    "                                            sequence_length=None,\n",
    "                                            dtype=tf.float32,\n",
    "                                            time_major=False)\n",
    "    \n",
    "    \n",
    "  \n",
    "    # Logistic layers, choose the last time step as inputs\n",
    "    h_inputs = rnn_outputs[:, -1, :]\n",
    "    \n",
    "    before_outputs = tf.contrib.layers.fully_connected(h_inputs, hidden_units, activation_fn=tf.nn.relu)\n",
    "    outputs = tf.contrib.layers.fully_connected(before_outputs, 1, activation_fn=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read IMDB dataset using Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 546s 31us/step\n",
      "17473536/17464789 [==============================] - 546s 31us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=10000,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)\n",
    "\n",
    "def generate_dataset(x_train, y_train, batch_num):\n",
    "    indices = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x_train, y_train = x_train[indices], y_train[indices]\n",
    "    xs = np.split(x_train, batch_num)\n",
    "    ys = np.split(y_train, batch_num)\n",
    "    return xs, ys\n",
    "\n",
    "  \n",
    "def max_len(sentences):\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    return sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(name='inputs', shape=[None, None], dtype=tf.int32)\n",
    "y = tf.placeholder(name='labels', shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Embedding matrix\n",
    "num_words = 10000\n",
    "embedding_size = 128\n",
    "embedding = tf.get_variable('Embedding', shape=[num_words, embedding_size], initializer=tf.random_uniform_initializer(-1., 1.))\n",
    "\n",
    "# Convert tokens into word vectors\n",
    "embedded = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "logits = rnn_forward(embedded)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "probs = tf.sigmoid(logits)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 0 \n",
      "Train loss and accuracy  0.703286 0.46 \n",
      "\n",
      "Epoch 0, step 50 \n",
      "Train loss and accuracy  0.62767 0.59 \n",
      "\n",
      "Epoch 0, step 100 \n",
      "Train loss and accuracy  0.389964 0.81 \n",
      "\n",
      "Epoch 0, step 150 \n",
      "Train loss and accuracy  0.471193 0.79 \n",
      "\n",
      "Epoch 0, step 200 \n",
      "Train loss and accuracy  0.25838 0.89 \n",
      "\n",
      "Epoch 1, step 0 \n",
      "Train loss and accuracy  0.234175 0.91 \n",
      "\n",
      "Epoch 1, step 50 \n",
      "Train loss and accuracy  0.112902 0.96 \n",
      "\n",
      "Epoch 1, step 100 \n",
      "Train loss and accuracy  0.1951 0.93 \n",
      "\n",
      "Epoch 1, step 150 \n",
      "Train loss and accuracy  0.177405 0.92 \n",
      "\n",
      "Epoch 1, step 200 \n",
      "Train loss and accuracy  0.204125 0.9 \n",
      "\n",
      "Epoch 2, step 0 \n",
      "Train loss and accuracy  0.168912 0.95 \n",
      "\n",
      "Epoch 2, step 50 \n",
      "Train loss and accuracy  0.136899 0.95 \n",
      "\n",
      "Epoch 2, step 100 \n",
      "Train loss and accuracy  0.169985 0.93 \n",
      "\n",
      "Epoch 2, step 150 \n",
      "Train loss and accuracy  0.21887 0.9 \n",
      "\n",
      "Epoch 2, step 200 \n",
      "Train loss and accuracy  0.193247 0.91 \n",
      "\n",
      "Test accuracy  0.84624 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 100\n",
    "batch_num = y_train.shape[0] / batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for k in range(num_epochs):\n",
    "        xs, ys = generate_dataset(x_train, y_train, batch_num)\n",
    "        for j in range(batch_num):\n",
    "            \n",
    "            # We can pass different time step at every iteration of dynamic RNN\n",
    "            maxlen = max_len(xs[j])\n",
    "            if maxlen > 100:\n",
    "                maxlen = 100\n",
    "                \n",
    "            X_feed = sequence.pad_sequences(xs[j], maxlen=maxlen)\n",
    "            y_feed = ys[j].reshape(batch_size, 1)\n",
    "            loss_val, _ , probs_val = sess.run([loss, train_op, probs], feed_dict={X: X_feed, y: y_feed})\n",
    "            \n",
    "            if j % 50 == 0:\n",
    "                predictions = [1 if i > 0.5 else 0 for i in probs_val]\n",
    "                accurate_pred = predictions == ys[j]\n",
    "                print \"Epoch %d, step %d \" % (k, j)\n",
    "                print \"Train loss and accuracy \", loss_val, float(sum(accurate_pred)) / batch_size, \"\\n\"\n",
    "                \n",
    "    # Test performance\n",
    "    test_xs, test_ys = generate_dataset(x_test, y_test, batch_num)\n",
    "    acc_count = []\n",
    "    for j in range(batch_num):\n",
    "    # We can pass different time step at every iteration of dynamic RNN\n",
    "        maxlen = max_len(test_xs[j])\n",
    "        if maxlen > 100:\n",
    "            maxlen = 100\n",
    "                \n",
    "        X_feed = sequence.pad_sequences(test_xs[j], maxlen=maxlen)\n",
    "        y_feed = test_ys[j].reshape(batch_size, 1)\n",
    "        probs_val = sess.run(probs, feed_dict={X: X_feed, y: y_feed})\n",
    "        predictions = [1 if i > 0.5 else 0 for i in probs_val]\n",
    "        accurate_pred = predictions == test_ys[j]\n",
    "        acc_count.append(sum(accurate_pred))\n",
    "    \n",
    "    print \"Test accuracy \", float(sum(acc_count)) / y_test.shape[0], \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
